---
source_name: "Custom Parse 2"
source_description: "x"
ingestion_parameters:
  file_mask: "parse2/Divvy_Station*.csv"
  delete_file: true
  mask_parsing: "glob"
  archive_folder: "archive"
  source_file_type: "single_file"
  disable_initiation: false
parsing_parameters:
  notebook_path: "/Shared/CustomParsing2"
  custom_parameters: {}
  force_case_insensitive: true
  fail_on_missing_columns: false
  lock_schema_new_columns: false
  lock_schema_datatype_changes: false
cdc_refresh_parameters: {}
alert_parameters:
  ingestion_failure_topics: []
  ingestion_success_topics: []
  data_processing_failure_topics: []
  data_processing_success_topics: []
file_type: "delimited"
refresh_type: "full"
connection_type: "file"
connection_name: "S3 ingest"
initiation_type: "scheduled"
cost_parameters:
  max_retries: 3
  cluster size: 2
  disable_profiling: false
  custom_cluster_params:
    libraries:
    - jar: "s3://test-datalake-wmp/dataops-sdk.jar"
    - jar: "s3://test-datalake-wmp/sparky.jar"
    new_cluster:
      spark_conf:
        spark.master: "local[*]"
        spark.databricks.cluster.profile: "singleNode"
        spark.databricks.service.server.enabled: "true"
      custom_tags:
        ResourceClass: "SingleNode"
      num_workers: 0
      spark_version: "7.3.x-scala2.12"
      aws_attributes:
        instance_profile_arn: "arn:aws:iam::496981143320:instance-profile/Test-db-instance-profile-WMP"
      instance_pool_id: "1012-220106-hoods9-pool-j7tl8h02"
  max_parallel_clusters: 1
  max_cluster_launch_wait: 300
  max_heartbeat_wait_time: 8
parser: "custom"
process_configuration_name: "MIGRATED  configuration for sources: 101"
custom_parse_cluster_configuration_name: "ACTION REQUIRED! Please Update based on\
  \ config in description! Migrated Custom Parse Notebook /Shared/CustomParsing2"
cleanup_configuration_name: "Default"
active_flag: true
create_datetime: "2021-12-15T15:13:07.214251"
created_userid: "jswanson@wmp.com"
update_datetime: "2021-12-15T16:03:37.394062"
updated_userid: "jswanson@wmp.com"
